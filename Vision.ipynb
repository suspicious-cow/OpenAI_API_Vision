{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univeral Code Used for the Entire Notebook\n",
    "\n",
    "Let's set up our libraries and client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the libraries we need for the notebook\n",
    "import base64  # For encoding and decoding binary data to/from ASCII\n",
    "import io  # For handling byte streams\n",
    "import os  # For interacting with the operating system\n",
    "import requests  # For making HTTP requests\n",
    "from PIL import Image as PILImage  # For image processing\n",
    "from IPython.display import display, Image, Markdown, clear_output  # For displaying content in Jupyter Notebooks\n",
    "from openai import OpenAI, AssistantEventHandler  # For OpenAI API and event handling\n",
    "from typing_extensions import override  # For method overriding in subclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision with Chat Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the image to be analyzed\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "\n",
    "# Making a request to the OpenAI API to analyze the image\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Whatâ€™s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url,\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the response from the OpenAI API\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Displaying the image in the notebook\n",
    "display(Image(url=image_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in Base64 Encoded Images (Image Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image to base64\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encodes an image to a base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"./artifacts/Hedy Lamarr and Frequency Hopping.png\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# Payload for the API request\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"How are these two images related?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Making a request to the OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the entire response from the API\n",
    "print(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extracting and printing the message content from the response\n",
    "message_content = response.choices[0].message.content\n",
    "print(message_content)\n",
    "\n",
    "# Decode the Base64 image data to binary\n",
    "image_data = base64.b64decode(base64_image)\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(Image(data=image_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Images as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the images to be analyzed\n",
    "image_url1 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/170207-N-WX059-0049_%2832005262704%29.jpg/537px-170207-N-WX059-0049_%2832005262704%29.jpg\"\n",
    "image_url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Dugong.JPG/640px-Dugong.JPG\"\n",
    "\n",
    "# Function to encode images to base64\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encodes an image to a base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"./artifacts/Hedy Lamarr and Frequency Hopping.png\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# Making a request to the OpenAI API to analyze the images\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in these images? Is there any difference between them?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url1,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url2,\n",
    "                    },\n",
    "                },\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the entire response from the API\n",
    "print(response)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extracting and printing the message content from the response\n",
    "message_content = response.choices[0].message.content\n",
    "print(message_content)\n",
    "\n",
    "# Display the images in the notebook\n",
    "display(Image(url=image_url1))\n",
    "display(Image(url=image_url2))\n",
    "\n",
    "# Decode the Base64 image data to binary\n",
    "image_data = base64.b64decode(base64_image)\n",
    "display(Image(data=image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision with Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Handler Setup for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler(AssistantEventHandler):\n",
    "    \"\"\"Custom event handler for processing assistant events.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.results = []  # Initialize an empty list to store the results\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        \"\"\"Handle the event when there is a text delta (partial text).\"\"\"\n",
    "        # Append the delta value (partial text) to the results list\n",
    "        self.results.append(delta.value)\n",
    "        # Call the method to update the Jupyter Notebook cell\n",
    "        self.update_output()\n",
    "\n",
    "    def update_output(self):\n",
    "        \"\"\"Update the Jupyter Notebook cell with the current markdown content.\"\"\"\n",
    "        # Clear the current output in the Jupyter Notebook cell\n",
    "        clear_output(wait=True)\n",
    "        # Join all the text fragments stored in results to form the complete markdown content\n",
    "        markdown_content = \"\".join(self.results)\n",
    "        # Display the markdown content in the Jupyter Notebook cell\n",
    "        display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an assistant using the client library.\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",  # Specify the model to be used.\n",
    "    \n",
    "    instructions=\"\"\" \n",
    "        You are a helpful assistant.\n",
    "    \"\"\",  # Instructions for the assistant.\n",
    "    \n",
    "    temperature=1,  # Set the temperature for response variability.\n",
    "    top_p=1,  # Set the top_p for nucleus sampling.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the image to be analyzed\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "\n",
    "# Create a new assistant thread with an initial user message\n",
    "assistant_thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url,\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the assistant's response\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=assistant_thread.id,  # Specify the thread ID.\n",
    "    assistant_id=assistant.id,  # Specify the assistant ID.\n",
    "    instructions=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    \"\"\",  # Provide instructions to the assistant.\n",
    "    event_handler=EventHandler(),  # Use the custom event handler for processing events.\n",
    ") as stream:\n",
    "    stream.until_done()  # Continue streaming until the assistant has finished responding.\n",
    "    \n",
    "# Display the images in the notebook\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your image\n",
    "image_path = \"./artifacts/Hedy Lamarr and Frequency Hopping.png\"\n",
    "\n",
    "# Upload the image file to the client for vision-related tasks\n",
    "image_file = client.files.create(\n",
    "    file=open(image_path, \"rb\"),\n",
    "    purpose=\"vision\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new assistant thread with an initial user message\n",
    "assistant_thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"How are these two images related?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_file\",\n",
    "                    \"image_file\": {\n",
    "                        \"file_id\": image_file.id\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the assistant's response\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=assistant_thread.id,  # Specify the thread ID.\n",
    "    assistant_id=assistant.id,  # Specify the assistant ID.\n",
    "    instructions=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    \"\"\",  # Provide instructions to the assistant.\n",
    "    event_handler=EventHandler(),  # Use the custom event handler for processing events.\n",
    ") as stream:\n",
    "    stream.until_done()  # Continue streaming until the assistant has finished responding.\n",
    "    \n",
    "# Retrieve the file content using the file ID\n",
    "image_data = client.files.content(image_file.id)\n",
    "\n",
    "# Create an image object from the binary data\n",
    "image = PILImage.open(io.BytesIO(image_data.content))\n",
    "\n",
    "# Display the image\n",
    "rgb_image = image.convert('RGB')\n",
    "display(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Images as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the first image to be analyzed\n",
    "image_url1 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/170207-N-WX059-0049_%2832005262704%29.jpg/537px-170207-N-WX059-0049_%2832005262704%29.jpg\"\n",
    "\n",
    "# Path to your local image\n",
    "image_path = \"./artifacts/Poker Hand.png\"\n",
    "\n",
    "# Upload the local image file to the client for vision-related tasks\n",
    "image_file = client.files.create(\n",
    "    file=open(image_path, \"rb\"),\n",
    "    purpose=\"vision\"\n",
    ")\n",
    "\n",
    "# Create a new assistant thread with an initial user message\n",
    "assistant_thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in these images?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url1\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_file\",\n",
    "                    \"image_file\": {\n",
    "                        \"file_id\": image_file.id\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the assistant's response\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=assistant_thread.id,  # Specify the thread ID.\n",
    "    assistant_id=assistant.id,  # Specify the assistant ID.\n",
    "    instructions=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    \"\"\",  # Provide instructions to the assistant.\n",
    "    event_handler=EventHandler(),  # Use the custom event handler for processing events.\n",
    ") as stream:\n",
    "    stream.until_done()  # Continue streaming until the assistant has finished responding.\n",
    "    \n",
    "# Display the image with the URL\n",
    "display(Image(url=image_url1))\n",
    "    \n",
    "# Display the image uploaded as a file\n",
    "# Retrieve the file content using the file ID\n",
    "image_data = client.files.content(image_file.id)\n",
    "\n",
    "# Create an image object from the binary data\n",
    "image = PILImage.open(io.BytesIO(image_data.content))\n",
    "\n",
    "# Display the image\n",
    "rgb_image = image.convert('RGB')\n",
    "display(rgb_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
